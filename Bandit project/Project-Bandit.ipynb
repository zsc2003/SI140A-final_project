{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Performance Evaluation of Bandit Algorithms\n",
    "\n",
    "- In this project, you will implement several classical bandit algorithms, evluate their performance via numerical comparison and finally gain inspiring intuition."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Classical Bandit Algorithms\n",
    "\n",
    "We consider a time-slotted bandit system ($t=1,2,\\ldots$) with three arms.\n",
    "We denote the arm set as $\\{1,2,3\\}$.\n",
    "Pulling each arm $j$ ($ j \\in \\{1,2,3\\}$) will obtain a random reward $r_{j}$, which follows a Bernoulli distribution with mean $\\theta_{j}$, *i.e.*, Bern($\\theta_{j}$).\n",
    "Specifically,\n",
    "\n",
    "\\begin{equation*}\n",
    "\t\\begin{aligned}\n",
    "\t\tr_{j} = \n",
    "\t\t\\begin{cases}\n",
    "\t\t\t1, & w.p.\\ \\theta_{j}, \\\\\n",
    "\t\t\t0, & w.p.\\ 1-\\theta_{j},\t\t\t\n",
    "\t\t\\end{cases}\n",
    "\t\\end{aligned}\n",
    "\\end{equation*}\n",
    "where $\\theta_{j}, j \\in\\{1,2,3\\}$ are parameters within $(0,1)$.\n",
    "  \n",
    "Now we run this bandit system for $N$ ($N \\gg 3$) time slots.\n",
    "In each time slot $t$, we choose one and only one arm from these three arms, which we denote as $I(t) \\in \\{1,2,3\\}$.\n",
    "Then we pull the arm $I(t)$ and obtain a random reward $r_{I(t)}$.\n",
    "Our objective is to find an optimal policy to choose an arm $I(t)$ in each time slot $t$ such that the expectation of the aggregated reward over $N$ time slots is maximized, *i.e.*,\n",
    "\n",
    "\\begin{equation*}\n",
    "\t\\begin{aligned}\n",
    "\t\t\\max_{I(t),t = 1,\\dots,N} \\ \\  \\mathbb{E}\\left[\\sum_{t=1}^{N} r_{I(t)} \\right].\n",
    "\t\\end{aligned}  \t\n",
    "\\end{equation*}\n",
    "\n",
    "If we know the values of $\\theta_{j},j \\in \\{1,2,3\\}$, this problem is trivial.\n",
    "Since $r_{I(t)} \\sim \\text{Bern}(\\theta_{I(t)})$,\n",
    "\n",
    "\\begin{equation*}\n",
    "\t\\begin{aligned}\n",
    "\t\t\\mathbb{E}\\left[\\sum_{t=1}^N r_{I(t)} \\right] \n",
    "\t\t= \\sum_{t=1}^{N} \\mathbb{E}[r_{I(t)}] \n",
    "\t\t= \\sum_{t=1}^N \\theta_{I(t)}.\n",
    "\t\\end{aligned} \t\n",
    "\\end{equation*}\n",
    "\n",
    "Let $I(t) = I^{*} = \\mathop{\\arg \\max}\\limits_{ j \\in \\{1,2,3\\}} \\ \\theta_j$ for $t=1,2,\\ldots,N$, then \n",
    "\n",
    "\\begin{equation*}\n",
    "\t\\begin{aligned}\n",
    "\t\t\\max_{I(t),t=1,\\ldots,N} \\ \\  \\mathbb{E}\\left[\\sum_{t=1}^N r_{I(t)} \\right] = N \\cdot \\theta_{I^*}.\n",
    "\t\\end{aligned} \t\n",
    "\\end{equation*}\n",
    "\n",
    "However, in reality, we do not know the values of $\\theta_{j},j \\in \\{1,2,3\\}$.\n",
    "We need to estimate the values $\\theta_{j}, j \\in \\{1,2,3\\}$ via empirical samples, and then make the decisions in each time slot. \n",
    "Next we introduce three classical bandit algorithms: $\\epsilon$-greedy, UCB, and TS, respectively."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\epsilon$-greedy Algorithm ($0 \\leq \\epsilon \\leq 1$)\n",
    "<img src=\"figures/e-greedy.jpg\" width=\"50%\" align='left'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UCB (Upper Confidence Bound) Algorithm\n",
    "<img src=\"figures/UCB.jpg\" width=\"50%\" align='left'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TS (Thompson Sampling) Algorithm\n",
    "<img src=\"figures/TS.jpg\" width=\"50%\" align='left'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Now suppose we obtain the parameters of the Bernoulli distributions from an oracle, which are shown in the following table. Choose $N=5000$ and compute the theoretically maximized expectation of aggregate rewards over $N$ time slots. We call it the oracle value. Note that these parameters $\\theta_{j}, j \\in \\{1,2,3\\}$ and oracle values are unknown to all bandit algorithms.\n",
    "\n",
    "| Arm $j$ | 1   | 2   | 3   |\n",
    "|---------|-----|-----|-----|\n",
    "| $\\theta_j$ | 0.7 | 0.5 | 0.4 |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your anwser of problem 1 in Part I**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Implement aforemented three classical bandit algorithms with following settings: \n",
    "   \n",
    "\t- $N=5000$\n",
    "\t- $\\epsilon$-greedy with $\\epsilon \\in \\{0.1, 0.5, 0.9\\}$.\n",
    "\t- UCB with $c \\in \\{1,5,10\\}$.\n",
    "\t- TS with\n",
    "    \t- $\\left\\{(\\alpha_1,\\beta_1)=(1,1),(\\alpha_2,\\beta_2)=(1,1),(\\alpha_3,\\beta_3)=(1,1)\\right\\}$ \n",
    "    \t- $\\left\\{(\\alpha_1,\\beta_1)=(601,401),(\\alpha_2,\\beta_2)=(401,601),(\\alpha_3,\\beta_3)=(2,3)\\right\\}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your anwser of problem 2 in Part I**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random, math, copy\n",
    "### Import more packages if you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Feel free to insert more blocks or helper functions if you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Implementation of epsilon-Greedy:\n",
    "### n is the number of time slots, epsilon is the parameter of the algorithm\n",
    "### return the total reward\n",
    "def greedy(n, epsilon):\n",
    "    # TODO\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Implementation of UCB Algorithm:\n",
    "### n is the number of time slots, c is the parameter of the algorithm\n",
    "### return the total reward\n",
    "def UCB(n, c):\n",
    "    # TODO\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Implementation of TS Algorithm\n",
    "### n is the number of time slots, a and b are the parameters of the algorithm\n",
    "### return the total reward\n",
    "def TS(n, a, b):\n",
    "    # TODO\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Regard each of the above setting in problem 2 of Part I as an experiment (in total $8$ experiments).\n",
    "Run each experiment $200$ independent trials (change the random seed).\n",
    "Plot the final result (in terms of rewards and regrets) averaged over these $200$ trials."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your anwser of problem 3 in Part I**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code for problem 1.3. Feel free to insert more blocks or helper functions if you need."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Compute the gaps between the algorithm outputs (aggregated rewards over $N$ time slots) and the oracle value. Compare the numerical results of $\\epsilon$-greedy, UCB, and TS.\n",
    "   - Which one is the best?\n",
    "   - Discuss the impacts of $\\epsilon$, $c$, and $\\alpha_{j}$, $\\beta_{j}$, respectively. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your anwser of problem 4 in Part I**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code for problem 1.4. Feel free to insert more blocks or helper functions if you need."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Give your understanding of the exploration-exploitation trade-off in bandit algorithms."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your anwser of problem 5 in Part I**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. We implicitly assume the reward distribution of these three arms are independent. How about the dependent case?\n",
    "\tCan you design an algorithm to exploit such information to obtain a better result?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your anwser of problem 6 in Part I**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code for problem 1.6. Feel free to insert more blocks or helper functions if you need."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Bayesian Bandit Algorithms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two arms which may be pulled repeatedly in any order.\n",
    "Each pull may result in either a success or a failure.\n",
    "The sequence of successes and failures which results from pulling arm $i$ ($i \\in \\{1, 2\\}$) forms a Bernoulli process with unknown success probability $\\theta_{i}$.\n",
    "A success at the $t^{th}$ pull yields a reward $\\gamma^{t-1}$ ($0 < \\gamma <1$), while an unsuccessful pull yields a zero reward.\n",
    "At time zero, each $\\theta_{i}$ has a Beta prior distribution with two parameters $\\alpha_{i}, \\beta_{i}$ and these distributions are independent for different arms.\n",
    "These prior distributions are updated to posterior distributions as arms are pulled.\n",
    "Since the class of Beta distributions is closed under Bernoulli sampling, posterior distributions are all Beta distributions.\n",
    "How should the arm to pull next in each time slot be chosen to maximize the total expected reward from an infinite sequence of pulls?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. \tOne intuitive policy suggests that in each time slot we should pull the arm for which the current expected value of $\\theta_{i}$ is the largest.\n",
    "\tThis policy behaves very good in most cases.\n",
    "\tPlease design simulations to check the behavior of this policy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your anwser of problem 1 in Part II**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code for problem 2.1. Feel free to insert more blocks or helper functions if you need."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. However, such intuitive policy is unfortunately not optimal.\n",
    "\tPlease provide an example to show why such policy is not optimal. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your anwser of problem 2 in Part II**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. For the expected total reward under an optimal policy, show that the following recurrence equation holds:\n",
    "\n",
    "\\begin{equation*}\n",
    "\t\t\\begin{aligned}\n",
    "\t\t\tR_{1}(\\alpha_{1},\\beta_{1}) \n",
    "\t\t\t= & \\frac{\\alpha_{1}}{\\alpha_{1}+\\beta_{1}} [1+\\gamma R(\\alpha_{1} + 1, \\beta_{1}, \\alpha_{2}, \\beta_{2})] \\\\\n",
    "\t\t\t\t& + \\frac{\\beta_{1}}{\\alpha_{1} + \\beta_{1}} [\\gamma R(\\alpha_{1}, \\beta_{1} + 1, \\alpha_{2}, \\beta_{2})]; \\\\\n",
    "\t\t\tR_{2}(\\alpha_{2}, \\beta_{2}) \n",
    "\t\t\t= & \\frac{\\alpha_{2}}{\\alpha_{2} + \\beta_{2}} [1 + \\gamma R(\\alpha_{1}, \\beta_{1}, \\alpha_{2} + 1, \\beta_{2})] \\\\\n",
    "\t\t\t\t& + \\frac{\\beta_{2}}{\\alpha_{2} + \\beta_{2}} [\\gamma R(\\alpha_{1}, \\beta_{1}, \\alpha_{2}, \\beta_{2} + 1)]; \\\\\n",
    "\t\t\tR(\\alpha_{1}, \\beta_{1}, \\alpha_{2}, \\beta_{2}) \n",
    "\t\t\t= & \\max \\left\\{ R_{1}(\\alpha_{1}, \\beta_{1}), R_{2}(\\alpha_{2}, \\beta_{2}) \\right\\}.\n",
    "\t\t\\end{aligned}  \t\n",
    "\t\\end{equation*}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your anwser of problem 3 in Part II**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. For the above equations, how to solve it exactly or approximately? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your anwser of problem 4 in Part II**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code for problem 2.4 if needed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Find the optimal policy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your anwser of problem 5 in Part II**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code for problem 2.5. Feel free to insert more blocks or helper functions if you need."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
