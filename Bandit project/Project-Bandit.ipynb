{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Performance Evaluation of Bandit Algorithms\n",
    "\n",
    "- In this project, you will implement several classical bandit algorithms, evluate their performance via numerical comparison and finally gain inspiring intuition."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Classical Bandit Algorithms\n",
    "\n",
    "We consider a time-slotted bandit system ($t=1,2,\\ldots$) with three arms.\n",
    "We denote the arm set as $\\{1,2,3\\}$.\n",
    "Pulling each arm $j$ ($ j \\in \\{1,2,3\\}$) will obtain a random reward $r_{j}$, which follows a Bernoulli distribution with mean $\\theta_{j}$, *i.e.*, Bern($\\theta_{j}$).\n",
    "Specifically,\n",
    "\n",
    "\\begin{equation*}\n",
    "\t\\begin{aligned}\n",
    "\t\tr_{j} = \n",
    "\t\t\\begin{cases}\n",
    "\t\t\t1, & w.p.\\ \\theta_{j}, \\\\\n",
    "\t\t\t0, & w.p.\\ 1-\\theta_{j},\t\t\t\n",
    "\t\t\\end{cases}\n",
    "\t\\end{aligned}\n",
    "\\end{equation*}\n",
    "where $\\theta_{j}, j \\in\\{1,2,3\\}$ are parameters within $(0,1)$.\n",
    "  \n",
    "Now we run this bandit system for $N$ ($N \\gg 3$) time slots.\n",
    "In each time slot $t$, we choose one and only one arm from these three arms, which we denote as $I(t) \\in \\{1,2,3\\}$.\n",
    "Then we pull the arm $I(t)$ and obtain a random reward $r_{I(t)}$.\n",
    "Our objective is to find an optimal policy to choose an arm $I(t)$ in each time slot $t$ such that the expectation of the aggregated reward over $N$ time slots is maximized, *i.e.*,\n",
    "\n",
    "\\begin{equation*}\n",
    "\t\\begin{aligned}\n",
    "\t\t\\max_{I(t),t = 1,\\dots,N} \\ \\  \\mathbb{E}\\left[\\sum_{t=1}^{N} r_{I(t)} \\right].\n",
    "\t\\end{aligned}  \t\n",
    "\\end{equation*}\n",
    "\n",
    "If we know the values of $\\theta_{j},j \\in \\{1,2,3\\}$, this problem is trivial.\n",
    "Since $r_{I(t)} \\sim \\text{Bern}(\\theta_{I(t)})$,\n",
    "\n",
    "\\begin{equation*}\n",
    "\t\\begin{aligned}\n",
    "\t\t\\mathbb{E}\\left[\\sum_{t=1}^N r_{I(t)} \\right] \n",
    "\t\t= \\sum_{t=1}^{N} \\mathbb{E}[r_{I(t)}] \n",
    "\t\t= \\sum_{t=1}^N \\theta_{I(t)}.\n",
    "\t\\end{aligned} \t\n",
    "\\end{equation*}\n",
    "\n",
    "Let $I(t) = I^{*} = \\mathop{\\arg \\max}\\limits_{ j \\in \\{1,2,3\\}} \\ \\theta_j$ for $t=1,2,\\ldots,N$, then \n",
    "\n",
    "\\begin{equation*}\n",
    "\t\\begin{aligned}\n",
    "\t\t\\max_{I(t),t=1,\\ldots,N} \\ \\  \\mathbb{E}\\left[\\sum_{t=1}^N r_{I(t)} \\right] = N \\cdot \\theta_{I^*}.\n",
    "\t\\end{aligned} \t\n",
    "\\end{equation*}\n",
    "\n",
    "However, in reality, we do not know the values of $\\theta_{j},j \\in \\{1,2,3\\}$.\n",
    "We need to estimate the values $\\theta_{j}, j \\in \\{1,2,3\\}$ via empirical samples, and then make the decisions in each time slot. \n",
    "Next we introduce three classical bandit algorithms: $\\epsilon$-greedy, UCB, and TS, respectively."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\epsilon$-greedy Algorithm ($0 \\leq \\epsilon \\leq 1$)\n",
    "<img src=\"figures/e-greedy.jpg\" width=\"50%\" align='left'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UCB (Upper Confidence Bound) Algorithm\n",
    "<img src=\"figures/UCB.jpg\" width=\"50%\" align='left'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TS (Thompson Sampling) Algorithm\n",
    "<img src=\"figures/TS.jpg\" width=\"50%\" align='left'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Now suppose we obtain the parameters of the Bernoulli distributions from an oracle, which are shown in the following table. Choose $N=5000$ and compute the theoretically maximized expectation of aggregate rewards over $N$ time slots. We call it the oracle value. Note that these parameters $\\theta_{j}, j \\in \\{1,2,3\\}$ and oracle values are unknown to all bandit algorithms.\n",
    "\n",
    "| Arm $j$ | 1   | 2   | 3   |\n",
    "|---------|-----|-----|-----|\n",
    "| $\\theta_j$ | 0.7 | 0.5 | 0.4 |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your anwser of problem 1 in Part I**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since each arm's parameter is oracled.\n",
    "\n",
    "So we just need to choose the arm with the largest parameter to have the maximum expectation of aggregate rewards over $N$ time slots.\n",
    "\n",
    "Since $\\theta_1 = 0.7, \\theta_2 = 0.5, \\theta_3 = 0.4$,\n",
    "\n",
    "so $\\theta_1 > \\theta_2 > \\theta_3$,\n",
    "\n",
    "so we choose arm 1 everytime.\n",
    "\n",
    "i.e. $$\\forall t, I(t)=I^*=\\argmax_{j\\in\\{1,2,3\\}}\\theta_j=1$$\n",
    "$$\\theta_{I(t)} = \\theta_1 = 0.7$$\n",
    "\n",
    "Also, since $r_{I(t)} \\sim \\text{Bern}(\\theta_{I(t)})$.\n",
    "\n",
    "So $E(r_{I(t)}) = \\theta_{I(t)}$.\n",
    "\n",
    "So the maximum expected value is \n",
    "$$\\max_{I(t),t=1,2,\\cdots,N}\\ E\\big[\\sum_{t=1}^Nr_{I(t)}\\big]$$\n",
    "$$=\\max_{I(t),t=1,2,\\cdots,N}\\ \\sum_{t=1}^NE\\big[r_{I(t)}\\big]$$\n",
    "$$=N \\cdot \\theta_{I^*} = 5000 \\times 0.7 = 3500$$\n",
    "\n",
    "So above all, with the given oracle parameters, the maximum expected value is 3500."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Implement aforemented three classical bandit algorithms with following settings: \n",
    "   \n",
    "\t- $N=5000$\n",
    "\t- $\\epsilon$-greedy with $\\epsilon \\in \\{0.1, 0.5, 0.9\\}$.\n",
    "\t- UCB with $c \\in \\{1,5,10\\}$.\n",
    "\t- TS with\n",
    "    \t- $\\left\\{(\\alpha_1,\\beta_1)=(1,1),(\\alpha_2,\\beta_2)=(1,1),(\\alpha_3,\\beta_3)=(1,1)\\right\\}$ \n",
    "    \t- $\\left\\{(\\alpha_1,\\beta_1)=(601,401),(\\alpha_2,\\beta_2)=(401,601),(\\alpha_3,\\beta_3)=(2,3)\\right\\}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your anwser of problem 2 in Part I**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random, math, copy\n",
    "### Import more packages if you need\n",
    "\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Feel free to insert more blocks or helper functions if you need.\n",
    "\n",
    "# since the arm's index are {1,2,3}\n",
    "# so we need to add a 0 at index 0\n",
    "# to make the index of arm's count and theta match the arm's index\n",
    "\n",
    "theta_oracled = [0, 0.7, 0.5, 0.4] # the oracled theta of each arm\n",
    "\n",
    "count = None\n",
    "theta = None\n",
    "\n",
    "def init_greedy():\n",
    "    global count, theta\n",
    "    count = [0, 0, 0, 0] # the initial count of each arm \n",
    "    theta = [0, 0, 0, 0] # the initial theta of each arm\n",
    "\n",
    "def init_UCB():\n",
    "    global count, theta\n",
    "    count = [0, 1, 1, 1] # the initial count of each arm \n",
    "    theta = [0, 0, 0, 0] # the initial theta of each arm\n",
    "    for t in range(1, 4):\n",
    "        arm = t\n",
    "        count[arm] = 1\n",
    "        r_i = np.random.binomial(1, theta_oracled[arm]) # R_I(t) ~ Bern(theta_oracled[I(t)])\n",
    "        theta[arm] = r_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Implementation of epsilon-Greedy:\n",
    "### n is the number of time slots, epsilon is the parameter of the algorithm\n",
    "### return the total reward\n",
    "def greedy(n, epsilon):\n",
    "    global count, theta\n",
    "    init_greedy() # initialize the count and theta of each arm\n",
    "\n",
    "    for t in range(1, n + 1): # the time slot\n",
    "        prob = random.random() # return value is in [0, 1)\n",
    "        arm = None # the arm to be chosen\n",
    "        if prob < epsilon: # explore (with probability epsilon)\n",
    "            arm = random.randint(1, 3) # randomly choose an arm from {1,2,3}\n",
    "        else: # exploit (with probability 1 - epsilon)\n",
    "            arm = np.argmax(theta) # choose the best arm\n",
    "            if arm == 0: # if this happened, it means that all the theta are 0\n",
    "                # so we can randomly choose an arm from {1,2,3}\n",
    "                arm = random.randint(1, 3) # randomly choose an arm from {1,2,3}\n",
    "        \n",
    "        # print(\"time slot: \", t, \" arm: \", arm)\n",
    "\n",
    "        r_i = np.random.binomial(1, theta_oracled[arm]) # r_i ~ Bern(theta_oracled[arm])\n",
    "\n",
    "        count[arm] += 1 # update the count of the chosen arm\n",
    "        theta[arm] += 1 / count[arm] * (r_i - theta[arm]) # update the theta of the chosen arm\n",
    "    \n",
    "    reward = count[1] * theta[1] + count[2] * theta[2] + count[3] * theta[3] # the expectation of the reward\n",
    "    return reward, regret # return the total reward and regret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Implementation of UCB Algorithm:\n",
    "### n is the number of time slots, c is the parameter of the algorithm\n",
    "### return the total reward\n",
    "def UCB(n, c):\n",
    "    global count, theta\n",
    "    init_UCB() # initialize the count and theta of each arm\n",
    "    \n",
    "    for t in range(4, n + 1):\n",
    "        arm = np.argmax([theta[i] + c * math.sqrt(2 * math.log(t) / count[i]) for i in range(1, 4)]) + 1\n",
    "        r_i = np.random.binomial(1, theta_oracled[arm]) # r_i ~ Bern(theta_oracled[arm])\n",
    "        \n",
    "        count[arm] += 1 # update the count of the chosen arm\n",
    "        theta[arm] += 1 / count[arm] * (r_i - theta[arm]) # update the theta of the chosen arm\n",
    "    \n",
    "    reward = count[1] * theta[1] + count[2] * theta[2] + count[3] * theta[3] # the expectation of the reward\n",
    "    return reward # return the total reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Implementation of TS Algorithm\n",
    "### n is the number of time slots, a and b are the parameters of the algorithm\n",
    "### return the total reward\n",
    "def TS(n, a, b):\n",
    "    reward = 0 # the expectation of the reward\n",
    "\n",
    "    for t in range(1, n + 1):\n",
    "        for i in range(1, 4):\n",
    "            theta[i] = np.random.beta(a[i], b[i]) # theta[i] ~ Beta(a[i], b[i])\n",
    "        arm = np.argmax(theta[1:4]) + 1 # choose the best arm\n",
    "        r_i = np.random.binomial(1, theta_oracled[arm]) # r_i ~ Bern(theta_oracled[arm])\n",
    "\n",
    "        a[arm] += r_i # update a[arm]\n",
    "        b[arm] += 1 - r_i # update b[arm]\n",
    "\n",
    "        reward += r_i # update the expectation of the reward\n",
    "    return reward"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Regard each of the above setting in problem 2 of Part I as an experiment (in total $8$ experiments).\n",
    "Run each experiment $200$ independent trials (change the random seed).\n",
    "Plot the final result (in terms of rewards and regrets) averaged over these $200$ trials."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your anwser of problem 3 in Part I**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:23<00:00,  8.46it/s]\n",
      "100%|██████████| 200/200 [00:19<00:00, 10.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3501.879999999999, 3496.2000000000007]\n",
      "[3501.879999999999, 3496.2000000000007]\n",
      "[3501.879999999999, 3496.2000000000007]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### Your code for problem 1.3. Feel free to insert more blocks or helper functions if you need.\n",
    "\n",
    "N = 5000\n",
    "repeat_time = 200\n",
    "\n",
    "# 1. the epsilon-greedy algoithm\n",
    "\"\"\"\n",
    "epsilon = [0.1, 0.5, 0.9]\n",
    "average = [0, 0, 0]\n",
    "\n",
    "for i in range(3):\n",
    "    for _ in tqdm.tqdm(range(repeat_time)):\n",
    "        average[i] += greedy(N, epsilon[i]) / repeat_time\n",
    "\n",
    "print(average)\n",
    "\"\"\"\n",
    "\n",
    "# 2. the UCB algorithm\n",
    "\"\"\"\n",
    "\n",
    "c = [1, 5, 10]\n",
    "average = [0, 0, 0]\n",
    "for i in range(3):\n",
    "    for _ in tqdm.tqdm(range(repeat_time)):\n",
    "        average[i] += UCB(N, c[i]) / repeat_time\n",
    "\n",
    "print(average)\n",
    "\"\"\"\n",
    "\n",
    "a = [[0 ,1, 1, 1], [0, 601, 401, 2]]\n",
    "b = [[0, 1, 1, 1], [0, 401, 601, 3]]\n",
    "average = [0, 0]\n",
    "for i in range(2):\n",
    "    for _ in tqdm.tqdm(range(repeat_time)):\n",
    "        average[i] += TS(N, a[i], b[i]) / repeat_time\n",
    "\n",
    "print(average)\n",
    "print(average)\n",
    "print(average)\n",
    "\n",
    "\n",
    "# TODO\n",
    "# print the reward and the regret!!!!!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Compute the gaps between the algorithm outputs (aggregated rewards over $N$ time slots) and the oracle value. Compare the numerical results of $\\epsilon$-greedy, UCB, and TS.\n",
    "   - Which one is the best?\n",
    "   - Discuss the impacts of $\\epsilon$, $c$, and $\\alpha_{j}$, $\\beta_{j}$, respectively. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your anwser of problem 4 in Part I**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code for problem 1.4. Feel free to insert more blocks or helper functions if you need."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Give your understanding of the exploration-exploitation trade-off in bandit algorithms."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your anwser of problem 5 in Part I**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Actually, initially I made some mistakes on understanding of the bandit algorithm.\n",
    "\n",
    "$\\hat{\\theta}_j$ is your evaluation of $\\Theta(j)$. In Bandit model, the paremeter of mean reward $\\theta$ is unknown, and it decides the reward you obtain. What you can know is your evaluation of $\\theta$ , which is $\\hat{\\theta}$ and you decide your choice according your evaluation $\\hat{\\theta}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. We implicitly assume the reward distribution of these three arms are independent. How about the dependent case?\n",
    "\tCan you design an algorithm to exploit such information to obtain a better result?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your anwser of problem 6 in Part I**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code for problem 1.6. Feel free to insert more blocks or helper functions if you need."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Bayesian Bandit Algorithms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two arms which may be pulled repeatedly in any order.\n",
    "Each pull may result in either a success or a failure.\n",
    "The sequence of successes and failures which results from pulling arm $i$ ($i \\in \\{1, 2\\}$) forms a Bernoulli process with unknown success probability $\\theta_{i}$.\n",
    "A success at the $t^{th}$ pull yields a reward $\\gamma^{t-1}$ ($0 < \\gamma <1$), while an unsuccessful pull yields a zero reward.\n",
    "At time zero, each $\\theta_{i}$ has a Beta prior distribution with two parameters $\\alpha_{i}, \\beta_{i}$ and these distributions are independent for different arms.\n",
    "These prior distributions are updated to posterior distributions as arms are pulled.\n",
    "Since the class of Beta distributions is closed under Bernoulli sampling, posterior distributions are all Beta distributions.\n",
    "How should the arm to pull next in each time slot be chosen to maximize the total expected reward from an infinite sequence of pulls?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. \tOne intuitive policy suggests that in each time slot we should pull the arm for which the current expected value of $\\theta_{i}$ is the largest.\n",
    "\tThis policy behaves very good in most cases.\n",
    "\tPlease design simulations to check the behavior of this policy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your anwser of problem 1 in Part II**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code for problem 2.1. Feel free to insert more blocks or helper functions if you need."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. However, such intuitive policy is unfortunately not optimal.\n",
    "\tPlease provide an example to show why such policy is not optimal. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your anwser of problem 2 in Part II**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "Please provide an example to show why such policy is not optimal\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. For the expected total reward under an optimal policy, show that the following recurrence equation holds:\n",
    "\n",
    "\\begin{equation*}\n",
    "\t\t\\begin{aligned}\n",
    "\t\t\tR_{1}(\\alpha_{1},\\beta_{1}) \n",
    "\t\t\t= & \\frac{\\alpha_{1}}{\\alpha_{1}+\\beta_{1}} [1+\\gamma R(\\alpha_{1} + 1, \\beta_{1}, \\alpha_{2}, \\beta_{2})] \\\\\n",
    "\t\t\t\t& + \\frac{\\beta_{1}}{\\alpha_{1} + \\beta_{1}} [\\gamma R(\\alpha_{1}, \\beta_{1} + 1, \\alpha_{2}, \\beta_{2})]; \\\\\n",
    "\t\t\tR_{2}(\\alpha_{2}, \\beta_{2}) \n",
    "\t\t\t= & \\frac{\\alpha_{2}}{\\alpha_{2} + \\beta_{2}} [1 + \\gamma R(\\alpha_{1}, \\beta_{1}, \\alpha_{2} + 1, \\beta_{2})] \\\\\n",
    "\t\t\t\t& + \\frac{\\beta_{2}}{\\alpha_{2} + \\beta_{2}} [\\gamma R(\\alpha_{1}, \\beta_{1}, \\alpha_{2}, \\beta_{2} + 1)]; \\\\\n",
    "\t\t\tR(\\alpha_{1}, \\beta_{1}, \\alpha_{2}, \\beta_{2}) \n",
    "\t\t\t= & \\max \\left\\{ R_{1}(\\alpha_{1}, \\beta_{1}), R_{2}(\\alpha_{2}, \\beta_{2}) \\right\\}.\n",
    "\t\t\\end{aligned}  \t\n",
    "\t\\end{equation*}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your anwser of problem 3 in Part II**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "prove the equation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. For the above equations, how to solve it exactly or approximately? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your anwser of problem 4 in Part II**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code for problem 2.4 if needed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Find the optimal policy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your anwser of problem 5 in Part II**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code for problem 2.5. Feel free to insert more blocks or helper functions if you need."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
